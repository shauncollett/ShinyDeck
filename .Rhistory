View(newdata)
set.seed(3434)
modFit <- train(Area ~ .,method="rpart",data=olive[,-1])
fancyRpartPlot(modFit$finalModel)
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata=newdata)
olive[,-1]
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[‐inTrain,]
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
set.seed(3434)
inTrain <- createDataPartition(y=olive$Area, p=0.7, list=FALSE)
training <- olive[inTrain,]
testing <- olive[-inTrain,]
modFit <- train(Area ~ .,method="rpart", data=training)
fancyRpartPlot(modFit$finalModel)
predict(modFit, newdata=newdata)
print(modFit$finalModel)
set.seed(3434)
modFit <- train(Area ~ ., method="rpart", data=olive)
fancyRpartPlot(modFit$finalModel)
# Then predict the value of area for the following data frame using the tree
# command with all defaults.
newdata = as.data.frame(t(colMeans(olive)))
predict(modFit, newdata=newdata)
2.78*2
mean(olive$Area)
library(tree)
tree1 <- tree(olive$Area ~ . , data=olive)
predict(tree1,newdata)
install.packages("tree")
library(tree)
tree1 <- tree(olive$Area ~ . , data=olive)
predict(tree1,newdata)
install.packages("ElemStatLearn")
rm(list=ls())
library(ElemStatLearn)
data(SAheart)
set.seed(8484)
train = sample(1:dim(SAheart)[1],size=dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
View(trainSA)
set.seed(13234)
glm1 <- glm(chd ~ age + alcohol + obesity + tobacco + typea + ldl, family="binomial", data=trainSA)
?train
modFit <- train(chd ~ age + alcohol + obesity + tobacco + typea + ldl,
method="glm", family="binomial", data=trainSA)
missClass(trainSA$chd, predict(modFit, trainSA))
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd, predict(modFit, trainSA))
missClass(testSA$chd, predict(modFit, testSA))
rm(list=ls())
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowel.train$y <- factor(vowl.train$y)
vowel.test$y <- factor(vowl.test$y)
set.seed(33833)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
modFit <‐ train(y ~ ., data=vowel.train, method="rf", prox=TRUE)
modFit <- train(y ~ ., data=vowel.train, method="rf", prox=TRUE)
?varImp
varImp(modFit)
URL <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
data <- read.csv(url, stringsAsFactors = FALSE)
library(RCurl)
URL <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
data <- read.csv(text = URL, stringsAsFactors = FALSE)
View(data)
rm(list=ls())
URL <- getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
training <- read.csv(text = URL, stringsAsFactors = FALSE)
View(training)
str(training)
str(training[,grepl("^cl",names(training))])
str(training[,grepl("^CL",names(training))])
str(training[,grepl("^C",names(training))])
str(training$classe)
summary(training$classe)
unique(training$classe)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
train <- vowel.train
test <- vowel.test
train$y <- as.factor(train$y)
test$y <- as.factor(test$y)
library(caret)
set.seed(33833)
modFit <- train(y ~ .,data=train,method="rf",prox=TRUE)
modFitRF <- train(y ~ .,data=train,method="rf",prox=TRUE)
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
library(caret)
train <- vowel.train
test <- vowel.test
train$y <- as.factor(train$y)
test$y <- as.factor(test$y)
set.seed(33833)
modFitRF <- train(y ~ ., data=train, method="rf", prox=TRUE)
modFitRF <- train(y ~ ., data=train, method="gbm", verbose=FALSE)
modFitGBM <- train(y ~ ., data=train, method="gbm", verbose=FALSE)
predTrainRF <- predict(modFitRF, test, type="class"
predTrainRF <- predict(modFitRF, test, type="class")
predTrainRF <- predict(modFitRF, test, type="class")
predTrainRF <- predict(modFitRF, test)
predTrainGBM <- predict(modFitGBM, test)
print(predTrainRF)
confusionMatrix(predTrainRF, test$y)
confusionMatrix(predTrainGBM, test$y)
predTrainRF <- predict(modFitRF, train)
predTrainGBM <- predict(modFitGBM, train)
confusionMatrix(predTrainRF, test$y)
confusionMatrix(predTrainRF, train$y)
confusionMatrix(predTrainGBM, train$y)
predTrainRF <- predict(modFitRF, test)
confusionMatrix(predTrainRF, test$y)
predTrainGBM <- predict(modFitGBM, test)
confusionMatrix(predTrainGBM, test$y)
modFitRF <- train(y ~ ., data=train[, -y], method="rf", prox=TRUE)
predTrainRF <- predict(modFitRF, train)
confusionMatrix(predTrainRF, train$y)
predTrainGBM <- predict(modFitGBM, train)
confusionMatrix(predTrainGBM, train$y)
predTestRF <- predict(modFitRF, test)
confusionMatrix(predTestRF, test$y)
predTestGBM <- predict(modFitGBM, test)
confusionMatrix(predTestGBM, test$y)
set.seed(33833)
modFitRF <- train(y ~ ., data=train, method="rf", prox=TRUE)
modFitGBM <- train(y ~ ., data=train, method="gbm", verbose=FALSE)
vowels <- data.frame(predTestRF, predTestGBM, y=test$y)
modFitJoin <- train(y ~ ., data=vowels, method="gam")
predTestJoin <- predict(modFitJoin, test)
confusionMatrix(predTestRF, test$y)
confusionMatrix(predTestGBM, test$y)
confusionMatrix(predTestJoin, vowels$y)
modFitRF <- train(y ~ ., data=train, method="rf", trControl = trainControl(number = 4))
modFitGBM <- train(y ~ ., data=train, method="gbm")
predTestRF <- predict(modFitRF, test)
predTestGBM <- predict(modFitGBM, test)
vowels <- data.frame(predTestRF, predTestGBM, y=test$y)
modFitJoin <- train(y ~ ., data=vowels, method="gam")
predTestJoin <- predict(modFitJoin, test)
vowel.train$y <- factor(vowel.train$y)
vowel.test$y <- factor(vowel.test$y)
set.seed(33833)
# create models
fit1 <- train(y ~ ., data = vowel.train, method = "rf", trControl = trainControl(number = 4))
fit2 <- train(y ~ ., data = vowel.train, method = "gbm")
# predict test
predict1 <- predict(fit1, newdata = vowel.test)
predict2 <- predict(fit2, newdata = vowel.test)
# combine predictions
DF_combined <- data.frame(predict1, predict2, y = vowel.test$y)
fit_combined <- train(y ~ ., data = DF_combined, method = "gam")
predict3 <- predict(fit_combined, newdata = vowel.test)
# confusion matrixes
c1 <- confusionMatrix(predict1, vowel.test$y)
c2 <- confusionMatrix(predict2, vowel.test$y)
c3 <- confusionMatrix(predict3, DF_combined$y)
c1
c2
rm(list=ls())
library(caret)
vowel.train$y <- as.factor(vowel.train$y)
vowel.test$y <- as.factor(vowel.test$y)
class(vowel.train$y)
set.seed(33833)
modFitRF <- train(y ~ ., data=train, method="rf")
modFitRF <- train(y ~ ., data=vowel.train, method="rf")
modFitGBM <- train(y ~ ., data=vowel.train, method="gbm")
predTestRF <- predict(modFitRF, vowel.test)
predTestGBM <- predict(modFitGBM, vowel.test)
confusionMatrix(predTestRF, vowel.test$y)
confusionMatrix(predTestGBM, vowel.test$y)
combined <- data.frame(predTestRF, predTestGBM, y=vowel.test$y)
modFitJoin <- train(y ~ ., data=combined, method="gam")
predTestJoin <- predict(modFitJoin, vowel.test)
confusionMatrix(predTestJoin, combined$y)
DF <- data.frame(predTestRF, predTestGBM)
View(DF)
DF$match <- DF$predTestRF == DF$predTestGBM
View(DF)
nrow(DF[DF$predTestRF == DF$predTestGBM,])
nrow(DF)
nrow(DF[DF$predTestRF == DF$predTestGBM,]) / nrow(DF)
confusionMatrix(predTestRF, vowel.test$y)$Accuracy
confusionMatrix(predTestRF, vowel.test$y)
cRF <- confusionMatrix(predTestRF, vowel.test$y)
confusionMatrix(predTestRF, vowel.test$y)$overall["Accuracy"]
confusionMatrix(predTestRF, vowel.test$y)$overall["Accuracy"]
confusionMatrix(predTestGBM, vowel.test$y)$overall["Accuracy"]
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
rm(list=ls())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
set.seed(62433)
modFitRF <- train(diagnosis ~ ., data=training, method="rf")
modFitGBM <- train(diagnosis ~ ., data=training, method="gbm")
modFitLDA <- train(diagnosis ~ ., data=training, method="lda")
predRF <- predict(modFitRF, testing)
predGBM <- predict(modFitGBM, testing)
predLDA <- predict(modFitLDA, testing)
predDF <- data.frame(predRF, predGBM, predLDA, diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~ ., method="rf", data=predDF)
combPred <- predict(combModFit, predDF)
confusionMatrix(predRF, testing$diagnosis)$overall["Accuracy"]
confusionMatrix(predRF, testing$diagnosis)$overall["Accuracy"]
confusionMatrix(predGBM, testing$diagnosis)$overall["Accuracy"]
confusionMatrix(predLDA, testing$diagnosis)$overall["Accuracy"]
confusionMatrix(combPred, testing$diagnosis)$overall["Accuracy"]
rm(list=ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
View(training)
set.seed(233)
modFit <- train(CompressiveStrength ~ ., data=training, method="lasso")
pred <- predict(modFit, testing)
?plot.enet
plot(pred, xvar="penalty")
plot(pred)
plot(modFit$finalModel, xvar="penalty", use.color=TRUE)
library(lubridate)  # For year() function below
dat = read.csv(
text = getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"),
stringsAsFactors = FALSE)
library(RCurl)
dat = read.csv(
text = getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"),
stringsAsFactors = FALSE)
rm(list=ls())
library(lubridate)  # For year() function below
library(RCurl)
dat = read.csv(
text = getURL("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv"),
stringsAsFactors = FALSE)
View(dat)
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
install.packages("forecast")
library(forecast)
?bats
fit <- bats(training)
View(training)
class(training$date)
View(training)
fit <- bats(tstrain)
?forecast
dim(testing)
View(testing)
nrow(testing)
fcast <- forecast(fit, level=95, h=nrow(testing))
accuracy(fcast, testing$visitsTumbler)
tstest <- ts(testing$visitsTumbler)
accuracy(fcast, tstest)
tstest <- ts(testing$visitsTumbler)
tstest <- ts(testing$visitsTumblr)
accuracy(fcast, tstest)
accuracy(fcast, testing$visitsTumblr)
t.test(fcast)
fcast$lower
df <- data.frame(fcast$lower, testing$visitsTubmlr, fcast$upper)
length(fcast$upper)
length(fcast$lower)
length(testing$visitsTumblr)
df <- data.frame(fcast$lower, testing$visitsTumblr, fcast$upper)
df$Within95 <- fcast$lower <= testing$visitsTumblr & testing$visitsTumblr >= fcast$upper
View(df)
df$Within95 <- fcast$lower >= testing$visitsTumblr & testing$visitsTumblr <= fcast$upper
View(df)
df$Within95 <- fcast$lower <= testing$visitsTumblr & testing$visitsTumblr <= fcast$upper
View(df)
length(df[Within95 == TRUE,])
length(df[df$Within95 == TRUE,])
nrow(df[df$Within95 == TRUE, ])
nrow(df[df$Within95 == TRUE, ]) / nrow(df)
rm(list=ls())
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(325)
install.packages("e1071")
install.packages("e1071")
install.packages("e1071")
install.packages("e1071")
library(e1071)
View(training)
fit <- svm(CompressiveStrength ~ ., data=training)
pred <- predict(fit, testing)
accuracy(pred, testing$CompressiveStrength)
accuracy(pred, testing$CompressiveStrength)
sqrt(sum((pred‐testing$CompressiveStrength)^2))
sqrt(sum((pred - testing$CompressiveStrength)^2))
rmse(pred, testing$CompressiveStrength)
sqrt(mean((pred - testing$CompressiveStrength)^2))
rm(list=ls())
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
# Set the seed to 62433 and predict diagnosis with all the other variables using
# a random forest ("rf"), boosted trees ("gbm") and linear discriminant analysis
# ("lda") model. Stack the predictions together using random forests ("rf").
# What is the resulting accuracy on the test set? Is it better or worse than
# each of the individual predictions?
set.seed(62433)
# create models
modFitRF <- train(diagnosis ~ ., data=training, method="rf")
modFitGBM <- train(diagnosis ~ ., data=training, method="gbm")
modFitLDA <- train(diagnosis ~ ., data=training, method="lda")
# predict on test set
predRF <- predict(modFitRF, testing)
predGBM <- predict(modFitGBM, testing)
predLDA <- predict(modFitLDA, testing)
# combining models
predDF <- data.frame(predRF, predGBM, predLDA, diagnosis=testing$diagnosis)
combModFit <- train(diagnosis ~ ., method="rf", data=predDF)
combPred <- predict(combModFit, predDF)
# find accuracy
confusionMatrix(predRF, testing$diagnosis)$overall["Accuracy"]
confusionMatrix(predGBM, testing$diagnosis)$overall["Accuracy"]
confusionMatrix(predLDA, testing$diagnosis)$overall["Accuracy"]
confusionMatrix(combPred, testing$diagnosis)$overall["Accuracy"]
install.packages("devtools")
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
library(manipulate)
myPlot <- function(s) {
plot(cars$dist - mean(cars$dist), cars$speed - mean(cars$speed))
abline(0, s)
}
manipulate(myPlot, s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
source('~/.active-rstudio-document', echo=TRUE)
manipulate(myPlot(s), x.s = slider(0, 2, step = 0.1))
manipulate(myPlot(s), slider = x(0, 2, step = 0.1))
manipulate(myPlot(s), s = slider(0, 2, step = 0.1))
install.packages("rcharts")
requires(rcharts)
require(rcharts)
?dtable
require(devtools)
install_github('rCharts', 'ramnathv')
?dtable
dataset(airquality)
head(airquality)
d <- data.frame(airquality, stringsAsFactors = FALSE) print(d)
d <- data.frame(airquality, stringsAsFactors = FALSE) print(d)
d <- data.frame(airquality, stringsAsFactors = FALSE)
print(d)
dTable(airquality, sPaginationType = "full_numbers")
library(rCharts)
data(airquality)
summary(airquality)
dTable(airquality, sPaginationType = "full_numbers")
d
install.packages("devtools")
library(devtools)
install_github('slidify','ramnathv')
install_github('slidifyLibraries','ramnathv')
library(slidify)
setwd("~/Box Sync/Play/datasciencecoursera/Developing Data Products/CourseraShinyApp/")
author("Shiny_MPG_App")
getwd()
slidify("index.rmd")
browseUrl(index.html)
browseURL(index.html)
browseURL("index.html")
data(mtcars)
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders"))
abline(lm(mpg ~ factor(cyl), mtcars), lwd = 2)
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg"))
abline(lm(mpg ~ factor(cyl), mtcars), lwd = 2)
abline(lm(mpg ~ factor(cyl) + wt, mtcars), lwd = 2)
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg"))
with(mtcars, plot(wt, mpg, xlab="weight", ylab="mpg"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg"))
abline(lm(mpg ~ factor(cyl), mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg",
title="Impact of Cylinders on MPG"))
abline(lm(mpg ~ factor(cyl), mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
?plot
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg",
main="Impact of Cylinders on MPG"))
abline(lm(mpg ~ factor(cyl), mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg",
main="Cylinder Impact on MPG"))
abline(lm(mpg ~ factor(cyl), mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg",
main="Weight Impact o MPG"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg",
main="Cylinder Impact on MPG"))
abline(lm(mpg ~ cyl, mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg",
main="Weight Impact o MPG"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg",
main="Cylinder Impact on MPG"))
abline(lm(mpg ~ factor(cyl), mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg",
main="Weight Impact o MPG"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
```
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(factor(cyl), mpg, xlab="cylinders", ylab="mpg",
main="Cylinder Impact on MPG"))
abline(lm(mpg ~ cyl, mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg",
main="Weight Impact o MPG"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(cyl, mpg, xlab="cylinders", ylab="mpg",
main="Cylinder Impact on MPG"))
abline(lm(mpg ~ cyl, mtcars), lwd = 2)
par(mfrow=c(1,2))
# Plot shows a clear negative relationship between cyl and mpg
with(mtcars, plot(cyl, mpg, xlab="cylinders", ylab="mpg",
main="Cylinder Impact on MPG"))
abline(lm(mpg ~ cyl, mtcars), lwd = 2)
# Weight: highly correlated negative slope.
with(mtcars, plot(wt, mpg, xlab="weight (tons)", ylab="mpg",
main="Weight Impact o MPG"))
abline(lm(mpg ~ wt, mtcars), lwd = 2)
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
slidify("index.rmd")
publish_github("shauncollett", "CourseraShinyApp")
publish_github("CourseraShinyApp", "shauncollett")
?publish_github
publish_github("CourseraShinyDeck", "shauncollett")
publish(user="shauncollett", repo="CourseraShinyApp")
